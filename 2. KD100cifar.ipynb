{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KD100cifar.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpal5Jee8Mfc",
        "colab_type": "text"
      },
      "source": [
        "# 2. Knowledge Distillation and Hint Layers on CIFAR-100\n",
        "This notebook contains the code for two experiments on CIFAR-100. The text accompanying the code is removed, as the code is completely equivalent to that of KD10cifar.ipynb, but then for CIFAR-100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhUzLQIg8LMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "#from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.models import Sequential, load_model, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, Lambda, concatenate\n",
        "from tensorflow.keras.losses import categorical_crossentropy as logloss\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq1MyFc8JIpw",
        "colab_type": "code",
        "outputId": "5e42a6a5-c232-4aa1-ca0b-f9e14174efa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "nb_classes = 100\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
        "\n",
        "# convert y_train and y_test to categorical binary values \n",
        "Y_train = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "X_train = X_train.reshape(50000, 32,32,3)\n",
        "X_test = X_test.reshape(10000, 32,32,3)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Normalize the values\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "input_shape = (32,32,3) # Input shape of each image\n",
        "\n",
        "# Hyperparameters\n",
        "nb_filters = 64 # number of convolutional filters to use\n",
        "pool_size = (2, 2) # size of pooling area for max pooling\n",
        "kernel_size = (3, 3) # convolution kernel size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 11s 0us/step\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g4aXGDLJLC_",
        "colab_type": "code",
        "outputId": "fcd8569d-4176-428b-c364-f162615442cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "t_conv1 = Conv2D(64, kernel_size=(3, 3),\n",
        "                 activation='relu', \n",
        "                 padding = 'same', \n",
        "                 kernel_initializer='he_normal')(inputs)\n",
        "t_conv2 = Conv2D(64, (3, 3), activation='relu', padding = 'same')(t_conv1)\n",
        "t_maxpool1 = MaxPooling2D(pool_size=(2, 2))(t_conv2)\n",
        "\n",
        "# Could add dropout here for reguralization\n",
        "\n",
        "t_conv3 = Conv2D(128, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 padding = 'same')(t_maxpool1)\n",
        "t_conv4 = Conv2D(128, (3, 3), activation='relu',padding = 'same')(t_conv3)\n",
        "t_maxpool2 = MaxPooling2D(pool_size=(2, 2))(t_conv4)\n",
        "\n",
        "\n",
        "t_conv5 = Conv2D(256, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 padding = 'same')(t_maxpool2)\n",
        "t_conv6 = Conv2D(256, (3, 3), activation='relu', padding = 'same')(t_conv5)\n",
        "t_conv7 = Conv2D(256, (3, 3), activation='relu', padding = 'same')(t_conv6)\n",
        "\n",
        "w_hint = Model(inputs=inputs, outputs = t_conv7, name = 'w_hint')\n",
        "\n",
        "t_maxpool3 = MaxPooling2D(pool_size=(2, 2))(t_conv7)\n",
        "\n",
        "t_do1 = Dropout(0.4)(t_maxpool3)\n",
        "\n",
        "t_conv8 = Conv2D(512, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 padding = 'same')(t_do1)\n",
        "t_conv9 = Conv2D(512, (3, 3), activation='relu', padding = 'same')(t_conv8)\n",
        "t_conv10 = Conv2D(512, (3, 3), activation='relu', padding = 'same')(t_conv9)\n",
        "t_conv11 = Conv2D(512, (3, 3), activation='relu', padding = 'same')(t_conv10)\n",
        "t_maxpool4 = MaxPooling2D(pool_size=(2,2))(t_conv11)\n",
        "\n",
        "t_do2 = Dropout(0.4)(t_maxpool4)\n",
        "\n",
        "t_flat = Flatten()(t_do2)\n",
        "t_dense1 = Dense(1024, activation='relu')(t_flat)\n",
        "t_dense2 = Dense(256, activation='relu')(t_dense1)\n",
        "t_dense3 = Dense(128, activation='relu')(t_dense2)\n",
        "\n",
        "t_do3 = Dropout(0.3)(t_dense3)\n",
        "\n",
        "t_dense_final = Dense(nb_classes, name = 'wo_softmax_teach')(t_do3)\n",
        "t_softmax = Activation('softmax')(t_dense_final)\n",
        "# Note that we add a normal softmax layer to begin with\n",
        "teacher = Model(inputs=inputs, outputs=t_softmax, name = 'teacher')\n",
        "\n",
        "t_optimizer = tf.keras.optimizers.SGD(\n",
        "    learning_rate=0.01, momentum=0.9, nesterov=True, name='SGD'\n",
        ")\n",
        "\n",
        "teacher.compile(loss='categorical_crossentropy',\n",
        "              optimizer=t_optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(teacher.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"teacher\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "wo_softmax_teach (Dense)     (None, 100)               12900     \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 100)               0         \n",
            "=================================================================\n",
            "Total params: 12,401,444\n",
            "Trainable params: 12,401,444\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykx3VH7kJVTn",
        "colab_type": "code",
        "outputId": "c2f14304-5520-4344-d51a-7d2a6e8079e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 40\n",
        "batch_size = 128\n",
        "teacher.fit(X_train, Y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, Y_test))\n",
        "# 0.5015"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 4.6055 - accuracy: 0.0096 - val_loss: 4.6046 - val_accuracy: 0.0088\n",
            "Epoch 2/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 4.5551 - accuracy: 0.0160 - val_loss: 4.4826 - val_accuracy: 0.0198\n",
            "Epoch 3/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 4.4543 - accuracy: 0.0210 - val_loss: 4.3916 - val_accuracy: 0.0256\n",
            "Epoch 4/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 4.3665 - accuracy: 0.0251 - val_loss: 4.2932 - val_accuracy: 0.0322\n",
            "Epoch 5/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 4.2432 - accuracy: 0.0285 - val_loss: 4.1707 - val_accuracy: 0.0378\n",
            "Epoch 6/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 4.1431 - accuracy: 0.0409 - val_loss: 4.1348 - val_accuracy: 0.0538\n",
            "Epoch 7/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 4.0165 - accuracy: 0.0598 - val_loss: 3.8417 - val_accuracy: 0.0847\n",
            "Epoch 8/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 3.8479 - accuracy: 0.0815 - val_loss: 3.6740 - val_accuracy: 0.1091\n",
            "Epoch 9/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 3.6695 - accuracy: 0.1088 - val_loss: 3.4834 - val_accuracy: 0.1445\n",
            "Epoch 10/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 3.4640 - accuracy: 0.1434 - val_loss: 3.2723 - val_accuracy: 0.1767\n",
            "Epoch 11/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 3.2589 - accuracy: 0.1793 - val_loss: 3.1286 - val_accuracy: 0.2095\n",
            "Epoch 12/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 3.0649 - accuracy: 0.2161 - val_loss: 2.8898 - val_accuracy: 0.2573\n",
            "Epoch 13/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 2.8778 - accuracy: 0.2524 - val_loss: 2.7127 - val_accuracy: 0.2911\n",
            "Epoch 14/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 2.6938 - accuracy: 0.2915 - val_loss: 2.6831 - val_accuracy: 0.2956\n",
            "Epoch 15/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 2.5269 - accuracy: 0.3236 - val_loss: 2.4443 - val_accuracy: 0.3544\n",
            "Epoch 16/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 2.3755 - accuracy: 0.3524 - val_loss: 2.3892 - val_accuracy: 0.3644\n",
            "Epoch 17/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 2.2435 - accuracy: 0.3820 - val_loss: 2.3334 - val_accuracy: 0.3783\n",
            "Epoch 18/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 2.1117 - accuracy: 0.4132 - val_loss: 2.2712 - val_accuracy: 0.3999\n",
            "Epoch 19/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.9882 - accuracy: 0.4422 - val_loss: 2.2832 - val_accuracy: 0.4061\n",
            "Epoch 20/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.8681 - accuracy: 0.4690 - val_loss: 2.2011 - val_accuracy: 0.4256\n",
            "Epoch 21/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.7623 - accuracy: 0.4982 - val_loss: 2.1102 - val_accuracy: 0.4514\n",
            "Epoch 22/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.6553 - accuracy: 0.5239 - val_loss: 2.1762 - val_accuracy: 0.4489\n",
            "Epoch 23/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.5629 - accuracy: 0.5463 - val_loss: 2.1522 - val_accuracy: 0.4594\n",
            "Epoch 24/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.4740 - accuracy: 0.5710 - val_loss: 2.1267 - val_accuracy: 0.4707\n",
            "Epoch 25/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.3860 - accuracy: 0.5918 - val_loss: 2.1578 - val_accuracy: 0.4590\n",
            "Epoch 26/40\n",
            "391/391 [==============================] - 12s 32ms/step - loss: 1.3165 - accuracy: 0.6100 - val_loss: 2.1018 - val_accuracy: 0.4787\n",
            "Epoch 27/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.2328 - accuracy: 0.6305 - val_loss: 2.1647 - val_accuracy: 0.4820\n",
            "Epoch 28/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.1717 - accuracy: 0.6523 - val_loss: 2.2528 - val_accuracy: 0.4650\n",
            "Epoch 29/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.1041 - accuracy: 0.6657 - val_loss: 2.2029 - val_accuracy: 0.4852\n",
            "Epoch 30/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 1.0378 - accuracy: 0.6859 - val_loss: 2.2278 - val_accuracy: 0.4891\n",
            "Epoch 31/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.9853 - accuracy: 0.7005 - val_loss: 2.3414 - val_accuracy: 0.4805\n",
            "Epoch 32/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.9384 - accuracy: 0.7142 - val_loss: 2.4223 - val_accuracy: 0.4848\n",
            "Epoch 33/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.8962 - accuracy: 0.7261 - val_loss: 2.3826 - val_accuracy: 0.4784\n",
            "Epoch 34/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.8420 - accuracy: 0.7434 - val_loss: 2.4032 - val_accuracy: 0.4951\n",
            "Epoch 35/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.8053 - accuracy: 0.7529 - val_loss: 2.4627 - val_accuracy: 0.4979\n",
            "Epoch 36/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.7615 - accuracy: 0.7669 - val_loss: 2.6055 - val_accuracy: 0.4873\n",
            "Epoch 37/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.7311 - accuracy: 0.7757 - val_loss: 2.5415 - val_accuracy: 0.4881\n",
            "Epoch 38/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.6894 - accuracy: 0.7892 - val_loss: 2.5000 - val_accuracy: 0.4839\n",
            "Epoch 39/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.6665 - accuracy: 0.7955 - val_loss: 2.6495 - val_accuracy: 0.4964\n",
            "Epoch 40/40\n",
            "391/391 [==============================] - 12s 31ms/step - loss: 0.6473 - accuracy: 0.8023 - val_loss: 2.4706 - val_accuracy: 0.4981\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd93ce2d0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pux1KuU7KB1l",
        "colab_type": "code",
        "outputId": "60bc805d-81fe-4f8f-b108-9504cb4b930c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "s_conv1 = Conv2D(16, kernel_size=(3, 3),\n",
        "                 activation='relu', \n",
        "                 padding = 'same', \n",
        "                 kernel_initializer='he_normal')(inputs)\n",
        "s_maxpool1 = MaxPooling2D(pool_size=(2, 2))(s_conv1)\n",
        "\n",
        "s_conv2 = Conv2D(32, (3, 3), activation='relu',padding='same')(s_maxpool1)\n",
        "s_maxpool2 = MaxPooling2D(pool_size=(2, 2))(s_conv2)\n",
        "\n",
        "w_guided =  Model(inputs=inputs, outputs=s_maxpool2, name = 'w_guided')\n",
        "\n",
        "s_flat1 = Flatten()(s_maxpool2)\n",
        "s_dense1 = Dense(128, activation='relu')(s_flat1)\n",
        "\n",
        "s_do1 = Dropout(0.3)(s_dense1)\n",
        "\n",
        "s_dense_final = Dense(nb_classes, name = 'wo_softmax_stud')(s_do1)\n",
        "s_softmax = Activation('softmax')(s_dense_final)\n",
        "\n",
        "student = Model(inputs=inputs, outputs=s_softmax, name = 'student')\n",
        "\n",
        "student.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "student.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"student\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 32, 32, 16)        448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "wo_softmax_stud (Dense)      (None, 100)               12900     \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 100)               0         \n",
            "=================================================================\n",
            "Total params: 280,260\n",
            "Trainable params: 280,260\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0mgTSwzNwK0",
        "colab_type": "code",
        "outputId": "899c0d44-f067-4011-f2a1-b163de4aa957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "student.fit(X_train, Y_train,\n",
        "          batch_size=64,\n",
        "          epochs=20,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, Y_test))\n",
        "# 0.3459, 0.3559, 0.3617, 0.3551 30 epochs\n",
        "# 0.3728 with hint and guided layer."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 4.0444 - accuracy: 0.0830 - val_loss: 3.4842 - val_accuracy: 0.1766\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 3.4846 - accuracy: 0.1662 - val_loss: 3.1752 - val_accuracy: 0.2344\n",
            "Epoch 3/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 3.2478 - accuracy: 0.2078 - val_loss: 3.0189 - val_accuracy: 0.2606\n",
            "Epoch 4/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 3.1030 - accuracy: 0.2345 - val_loss: 2.8563 - val_accuracy: 0.2927\n",
            "Epoch 5/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.9923 - accuracy: 0.2520 - val_loss: 2.8161 - val_accuracy: 0.3087\n",
            "Epoch 6/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.9086 - accuracy: 0.2679 - val_loss: 2.7315 - val_accuracy: 0.3214\n",
            "Epoch 7/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.8290 - accuracy: 0.2832 - val_loss: 2.7073 - val_accuracy: 0.3199\n",
            "Epoch 8/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.7783 - accuracy: 0.2941 - val_loss: 2.6774 - val_accuracy: 0.3348\n",
            "Epoch 9/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.7171 - accuracy: 0.3040 - val_loss: 2.6505 - val_accuracy: 0.3342\n",
            "Epoch 10/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.6684 - accuracy: 0.3138 - val_loss: 2.6418 - val_accuracy: 0.3371\n",
            "Epoch 11/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.6361 - accuracy: 0.3204 - val_loss: 2.6132 - val_accuracy: 0.3464\n",
            "Epoch 12/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.5889 - accuracy: 0.3291 - val_loss: 2.6141 - val_accuracy: 0.3423\n",
            "Epoch 13/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.5496 - accuracy: 0.3404 - val_loss: 2.6165 - val_accuracy: 0.3409\n",
            "Epoch 14/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.5187 - accuracy: 0.3435 - val_loss: 2.5866 - val_accuracy: 0.3494\n",
            "Epoch 15/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.4923 - accuracy: 0.3489 - val_loss: 2.5881 - val_accuracy: 0.3472\n",
            "Epoch 16/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.4569 - accuracy: 0.3525 - val_loss: 2.5896 - val_accuracy: 0.3508\n",
            "Epoch 17/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.4285 - accuracy: 0.3562 - val_loss: 2.5926 - val_accuracy: 0.3487\n",
            "Epoch 18/20\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 2.4041 - accuracy: 0.3640 - val_loss: 2.5815 - val_accuracy: 0.3551\n",
            "Epoch 19/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.3857 - accuracy: 0.3686 - val_loss: 2.5999 - val_accuracy: 0.3511\n",
            "Epoch 20/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 2.3557 - accuracy: 0.3727 - val_loss: 2.6005 - val_accuracy: 0.3528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbba4275da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4FWFpmFLYsT",
        "colab_type": "code",
        "outputId": "b85a5158-cad7-4fa2-e0e9-b2e2ddcccafd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "conv_regressor = Conv2D(256, kernel_size=(1, 1),\n",
        "                 input_shape = (8,8,32),\n",
        "                 kernel_initializer='glorot_normal')(w_guided.output)\n",
        "\n",
        "w_r = Model(inputs=inputs, outputs=conv_regressor, name = 'w_r')\n",
        "print(w_hint.output)\n",
        "print(w_r.output)\n",
        "print(teacher.output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"conv2d_6/Identity:0\", shape=(None, 8, 8, 256), dtype=float32)\n",
            "Tensor(\"conv2d_15/Identity:0\", shape=(None, 8, 8, 256), dtype=float32)\n",
            "Tensor(\"activation/Identity:0\", shape=(None, 100), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyNF73ZNLad5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fitnet_loss(target_feat, source_feat):\n",
        "    return tf.reduce_mean(tf.square(target_feat-source_feat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e8y84sxLbts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w_r.compile(\n",
        "    optimizer='adam',\n",
        "    loss= (lambda y_hint, y_guided: fitnet_loss(y_hint, y_guided)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy0D0aDsLeWL",
        "colab_type": "code",
        "outputId": "6c930089-a99f-4a78-c372-e674755943fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w_hint_outputs = w_hint.predict(X_train)\n",
        "print(w_hint_outputs.shape)\n",
        "#np.save(\"w_hint_outputs.npy\",w_hint_outputs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 8, 8, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sD92Fl_Lfz0",
        "colab_type": "code",
        "outputId": "1dc521e5-7a85-4e78-cdcf-bf9a03b05fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "w_r.fit(X_train, w_hint_outputs,\n",
        "          batch_size=128,\n",
        "          epochs=10)#,\n",
        "          #verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1104\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1099\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1095\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1093\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1091\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1090\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1089\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1089\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1088\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 0.1087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd8c8692208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iawKiPbiLh-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a manual softmax function\n",
        "def softmax(x):\n",
        "    return np.exp(x)/(np.exp(x).sum())\n",
        "    \n",
        "#teacher_WO_Softmax = Model(teacher.input, teacher.get_layer('wo_softmax_teach').output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77yJzXBFLjgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_train_logits = teacher_WO_Softmax.predict(X_train)\n",
        "teacher_test_logits = teacher_WO_Softmax.predict(X_test) \n",
        "# This model directly gives the logits ( see the teacher_WO_softmax model above)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS6PUcz_Ll6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"teacher_train_logits_100_cifar.npy\",teacher_train_logits)\n",
        "np.save(\"teacher_test_logits_100_cifar.npy\",teacher_test_logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eFMRqHXLmag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_train_logits = np.load(\"teacher_train_logits_100_cifar.npy\")\n",
        "teacher_test_logits = np.load(\"teacher_test_logits_100_cifar.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVfNI1GHPT-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_train_logits = teacher_train_logits.astype('float64')\n",
        "teacher_test_logits = teacher_test_logits.astype('float64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EphWLIB3Lord",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set a tempature value\n",
        "temp = 10\n",
        "\n",
        "\n",
        "# Perform a manual softmax at raised temperature\n",
        "train_logits_T = teacher_train_logits / temp\n",
        "test_logits_T = teacher_test_logits / temp \n",
        "\n",
        "Y_train_soft = softmax(train_logits_T)\n",
        "Y_test_soft = softmax(test_logits_T)\n",
        "\n",
        "# Concatenate so that this becomes a 10 + 10 dimensional vector\n",
        "Y_train_new = np.concatenate([Y_train, Y_train_soft], axis=1)\n",
        "Y_test_new =  np.concatenate([Y_test, Y_test_soft], axis =1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBwnT8zILqjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = student.layers[-2].output # This is going to be a tensor. And hence it needs to pass through a Activation layer\n",
        "probs = Activation('softmax')(logits)\n",
        "\n",
        "# softened probabilities at raised temperature\n",
        "logits_T = tf.keras.layers.Lambda(lambda x: x / temp)(logits)\n",
        "probs_T = Activation('softmax')(logits_T)\n",
        "\n",
        "output = tf.keras.layers.concatenate([probs, probs_T])\n",
        "\n",
        "# This is our new student model\n",
        "student_kd = Model(student.input, output)\n",
        "\n",
        "#student_kd.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7prTqO2Lrmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declare knowledge distillation loss\n",
        "def knowledge_distillation_loss(y_true, y_pred, alpha):\n",
        "\n",
        "    # Extract the one-hot encoded values and the softs separately so that we can create two objective functions\n",
        "    y_true, y_true_softs = y_true[: , :nb_classes], y_true[: , nb_classes:]\n",
        "    \n",
        "    y_pred, y_pred_softs = y_pred[: , :nb_classes], y_pred[: , nb_classes:]\n",
        "    \n",
        "    loss = (1-alpha)*logloss(y_true,y_pred) + alpha*logloss(y_true_softs, y_pred_softs)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# For testing use regular output probabilities - without temperature\n",
        "def acc(y_true, y_pred):\n",
        "    y_true = y_true[:, :nb_classes]\n",
        "    y_pred = y_pred[:, :nb_classes]\n",
        "    return tf.keras.metrics.categorical_accuracy(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "titXpkSMLsnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "student_kd.compile(\n",
        "    optimizer='adam',\n",
        "    loss=(lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, 0.5)),\n",
        "    metrics=[acc] )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRu4pMafLuUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "student_kd.fit(X_train, Y_train_new,\n",
        "          batch_size=64,\n",
        "          epochs=1,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, Y_test_new))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttQSTaRZLvgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "student_kd.reset_states()\n",
        "student.reset_states()\n",
        "student_kd = None\n",
        "student = None\n",
        "del student_kd\n",
        "del student\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}