# A knowledge distillation approach to increase the accuracy of lightweight convolutional neural networks
Predicting data using complex models is cumbersome and can be too computationally expensive to allow deployment to many users, especially if the complex models are large neural networks. This paper aims to achieve high accuracy on image classification tasks using small and fast-to-execute models. These models do not in themselves achieve high accuracy, but rather require the more complex models to train them. This is achieved using a combination of knowledge distillation, a technique that distills knowledge from the output of a complex teacher network to a small student network, and hint layers, a technique that distills knowledge from the output of an intermediate layer from a complex teacher network in a similar fashion. The performance gains are measurable but not considerable.