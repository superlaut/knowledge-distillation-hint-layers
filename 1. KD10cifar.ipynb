{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbH1p5308aEC"
   },
   "source": [
    "# 1. Knowledge Distillation and Hint Layers on CIFAR-10\n",
    "This notebook contains the code for two experiments on CIFAR-10. The text accompanying the code is brief, as most of the explanation is in the main pdf-file.\n",
    "\n",
    "First, the packages are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-qEMSUA88ZiT",
    "outputId": "b41c861b-7b7b-4020-c20f-20eb2fcf207e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, Lambda, concatenate\n",
    "from tensorflow.keras.losses import categorical_crossentropy as logloss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89o9oKH987M4"
   },
   "source": [
    "In the code below the CIFAR-10 data is imported and normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "1OxBpGGeqRGe",
    "outputId": "c41b8eab-5775-4259-de28-64d48705205f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 11s 0us/step\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# convert y_train and y_test to categorical binary values \n",
    "Y_train = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train = X_train.reshape(50000, 32,32,3)\n",
    "X_test = X_test.reshape(10000, 32,32,3)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize the values\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "input_shape = (32,32,3) # Input shape of each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuO8R5tp9E-g"
   },
   "source": [
    "In the code block below a CNN with an architecture similar to VGG is created and explained in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "E4VbVztUqVw_",
    "outputId": "c587f87a-3c13-4dd6-d816-3fbcf4b0c682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "wo_softmax_teach (Dense)     (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 12,372,746\n",
      "Trainable params: 12,372,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "t_conv1 = Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu', \n",
    "                 padding = 'same', \n",
    "                 kernel_initializer='he_normal')(inputs)\n",
    "t_conv2 = Conv2D(64, (3, 3), activation='relu', padding = 'same')(t_conv1)\n",
    "t_maxpool1 = MaxPooling2D(pool_size=(2, 2))(t_conv2)\n",
    "\n",
    "# No dropout added here, as the hint layer is only created later, and adding dropout in between\n",
    "# might not be smart\n",
    "\n",
    "t_conv3 = Conv2D(128, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 padding = 'same')(t_maxpool1)\n",
    "t_conv4 = Conv2D(128, (3, 3), activation='relu',padding='same')(t_conv3)\n",
    "t_maxpool2 = MaxPooling2D(pool_size=(2, 2))(t_conv4)\n",
    "\n",
    "\n",
    "t_conv5 = Conv2D(256, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 padding = 'same')(t_maxpool2)\n",
    "t_conv6 = Conv2D(256, (3, 3), activation='relu', padding = 'same')(t_conv5)\n",
    "t_conv7 = Conv2D(256, (3, 3), activation='relu', padding = 'same')(t_conv6)\n",
    "\n",
    "# Create the hint 'model'\n",
    "w_hint = Model(inputs=inputs, outputs = t_conv7, name = 'w_hint')\n",
    "\n",
    "t_maxpool3 = MaxPooling2D(pool_size=(2, 2))(t_conv7)\n",
    "\n",
    "t_do1 = Dropout(0.4)(t_maxpool3)\n",
    "\n",
    "t_conv8 = Conv2D(512, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 padding = 'same')(t_do1)\n",
    "t_conv9 = Conv2D(512, (3, 3), activation='relu', padding = 'same')(t_conv8)\n",
    "t_conv10 = Conv2D(512, (3, 3), activation='relu', padding = 'same')(t_conv9)\n",
    "t_conv11 = Conv2D(512, (3, 3), activation='relu', padding = 'same')(t_conv10)\n",
    "t_maxpool4 = MaxPooling2D(pool_size=(2, 2))(t_conv11)\n",
    "\n",
    "t_do2 = Dropout(0.4)(t_maxpool4)\n",
    "\n",
    "t_flat = Flatten()(t_do2)\n",
    "t_dense1 = Dense(1024, activation='relu')(t_flat)\n",
    "t_dense2 = Dense(256, activation='relu')(t_dense1)\n",
    "t_dense3 = Dense(64, activation='relu')(t_dense2)\n",
    "\n",
    "t_do3 = Dropout(0.3)(t_dense3)\n",
    "\n",
    "t_dense_final = Dense(nb_classes, name = 'wo_softmax_teach')(t_do3)\n",
    "t_softmax = Activation('softmax')(t_dense_final)\n",
    "# Note that we add a normal softmax layer to begin with\n",
    "teacher = Model(inputs=inputs, outputs=t_softmax, name = 'teacher')\n",
    "\n",
    "\n",
    "# momentum optimiser\n",
    "t_optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01, momentum=0.9, nesterov=True, name='SGD'\n",
    ")\n",
    "\n",
    "teacher.compile(loss='categorical_crossentropy',\n",
    "              optimizer=t_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "print(teacher.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jyxEm3j4qqXh",
    "outputId": "0629af5b-55aa-4ec7-8cbe-5f772eadefc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 2.2947 - accuracy: 0.1109 - val_loss: 2.3021 - val_accuracy: 0.1030\n",
      "Epoch 2/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2.2632 - accuracy: 0.1393 - val_loss: 2.0640 - val_accuracy: 0.2575\n",
      "Epoch 3/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.9529 - accuracy: 0.2666 - val_loss: 1.7121 - val_accuracy: 0.3575\n",
      "Epoch 4/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.6594 - accuracy: 0.3724 - val_loss: 1.4987 - val_accuracy: 0.4251\n",
      "Epoch 5/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.4129 - accuracy: 0.4811 - val_loss: 1.2276 - val_accuracy: 0.5484\n",
      "Epoch 6/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.2063 - accuracy: 0.5685 - val_loss: 1.2954 - val_accuracy: 0.5445\n",
      "Epoch 7/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.0295 - accuracy: 0.6352 - val_loss: 0.9717 - val_accuracy: 0.6547\n",
      "Epoch 8/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.8891 - accuracy: 0.6908 - val_loss: 0.8446 - val_accuracy: 0.7062\n",
      "Epoch 9/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7859 - accuracy: 0.7273 - val_loss: 0.7905 - val_accuracy: 0.7272\n",
      "Epoch 10/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6927 - accuracy: 0.7616 - val_loss: 0.7101 - val_accuracy: 0.7589\n",
      "Epoch 11/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6103 - accuracy: 0.7951 - val_loss: 0.6735 - val_accuracy: 0.7696\n",
      "Epoch 12/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.5434 - accuracy: 0.8193 - val_loss: 0.6230 - val_accuracy: 0.7997\n",
      "Epoch 13/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.4800 - accuracy: 0.8399 - val_loss: 0.6545 - val_accuracy: 0.7935\n",
      "Epoch 14/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.4266 - accuracy: 0.8571 - val_loss: 0.7068 - val_accuracy: 0.7832\n",
      "Epoch 15/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3770 - accuracy: 0.8757 - val_loss: 0.6221 - val_accuracy: 0.8108\n",
      "Epoch 16/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3357 - accuracy: 0.8890 - val_loss: 0.6141 - val_accuracy: 0.8134\n",
      "Epoch 17/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2990 - accuracy: 0.9008 - val_loss: 0.6528 - val_accuracy: 0.8132\n",
      "Epoch 18/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2713 - accuracy: 0.9112 - val_loss: 0.6974 - val_accuracy: 0.8175\n",
      "Epoch 19/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2377 - accuracy: 0.9230 - val_loss: 0.6635 - val_accuracy: 0.8248\n",
      "Epoch 20/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2120 - accuracy: 0.9306 - val_loss: 0.6467 - val_accuracy: 0.8220\n",
      "Epoch 21/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1923 - accuracy: 0.9369 - val_loss: 0.7266 - val_accuracy: 0.8065\n",
      "Epoch 22/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1777 - accuracy: 0.9420 - val_loss: 0.6663 - val_accuracy: 0.8249\n",
      "Epoch 23/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1504 - accuracy: 0.9507 - val_loss: 0.9172 - val_accuracy: 0.8045\n",
      "Epoch 24/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1431 - accuracy: 0.9530 - val_loss: 0.8017 - val_accuracy: 0.8206\n",
      "Epoch 25/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1247 - accuracy: 0.9599 - val_loss: 0.7661 - val_accuracy: 0.8277\n",
      "Epoch 26/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1116 - accuracy: 0.9626 - val_loss: 0.9210 - val_accuracy: 0.8137\n",
      "Epoch 27/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1068 - accuracy: 0.9652 - val_loss: 0.8273 - val_accuracy: 0.8331\n",
      "Epoch 28/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0944 - accuracy: 0.9704 - val_loss: 0.8013 - val_accuracy: 0.8336\n",
      "Epoch 29/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0916 - accuracy: 0.9703 - val_loss: 0.8494 - val_accuracy: 0.8262\n",
      "Epoch 30/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0818 - accuracy: 0.9738 - val_loss: 0.8394 - val_accuracy: 0.8325\n",
      "Epoch 31/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0723 - accuracy: 0.9768 - val_loss: 0.8492 - val_accuracy: 0.8262\n",
      "Epoch 32/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0742 - accuracy: 0.9762 - val_loss: 0.8513 - val_accuracy: 0.8292\n",
      "Epoch 33/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0708 - accuracy: 0.9767 - val_loss: 0.8593 - val_accuracy: 0.8317\n",
      "Epoch 34/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0597 - accuracy: 0.9806 - val_loss: 0.9190 - val_accuracy: 0.8314\n",
      "Epoch 35/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0591 - accuracy: 0.9807 - val_loss: 0.8810 - val_accuracy: 0.8336\n",
      "Epoch 36/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0584 - accuracy: 0.9817 - val_loss: 0.9397 - val_accuracy: 0.8241\n",
      "Epoch 37/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0598 - accuracy: 0.9811 - val_loss: 0.8864 - val_accuracy: 0.8301\n",
      "Epoch 38/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0480 - accuracy: 0.9849 - val_loss: 0.8992 - val_accuracy: 0.8325\n",
      "Epoch 39/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0440 - accuracy: 0.9866 - val_loss: 0.9567 - val_accuracy: 0.8376\n",
      "Epoch 40/40\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0488 - accuracy: 0.9849 - val_loss: 0.9666 - val_accuracy: 0.8256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf6f660dd8>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the teacher\n",
    "epochs = 40\n",
    "batch_size = 128\n",
    "teacher.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "45i9ex4W9oNw"
   },
   "source": [
    "In the code below the student network is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "XXytMxhKqgiJ",
    "outputId": "0780ca5b-4ebf-4587-8dc5-21451818956c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"student\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "wo_softmax_stud (Dense)      (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 268,650\n",
      "Trainable params: 268,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "s_conv1 = Conv2D(16, kernel_size=(3, 3),\n",
    "                 activation='relu', \n",
    "                 padding = 'same', \n",
    "                 kernel_initializer='he_normal')(inputs)\n",
    "s_maxpool1 = MaxPooling2D(pool_size=(2, 2))(s_conv1)\n",
    "\n",
    "s_conv2 = Conv2D(32, (3, 3), activation='relu',padding='same')(s_maxpool1)\n",
    "s_maxpool2 = MaxPooling2D(pool_size=(2, 2))(s_conv2)\n",
    "\n",
    "# Guided 'model'\n",
    "w_guided =  Model(inputs=inputs, outputs=s_maxpool2, name = 'w_guided')\n",
    "\n",
    "s_flat1 = Flatten()(s_maxpool2)\n",
    "s_dense1 = Dense(128, activation='relu')(s_flat1)\n",
    "\n",
    "s_do1 = Dropout(0.3)(s_dense1)\n",
    "\n",
    "s_dense_final = Dense(nb_classes, name = 'wo_softmax_stud')(s_do1)\n",
    "s_softmax = Activation('softmax')(s_dense_final)\n",
    "\n",
    "student = Model(inputs=inputs, outputs=s_softmax, name = 'student')\n",
    "\n",
    "student.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "student.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aODJv0pYsfEe",
    "outputId": "96703900-67e6-4de7-8036-af0a44014133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6039 - accuracy: 0.4158 - val_loss: 1.2752 - val_accuracy: 0.5387\n",
      "Epoch 2/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2949 - accuracy: 0.5373 - val_loss: 1.1566 - val_accuracy: 0.5855\n",
      "Epoch 3/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1706 - accuracy: 0.5855 - val_loss: 1.0758 - val_accuracy: 0.6209\n",
      "Epoch 4/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0901 - accuracy: 0.6128 - val_loss: 1.0384 - val_accuracy: 0.6347\n",
      "Epoch 5/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0327 - accuracy: 0.6343 - val_loss: 0.9789 - val_accuracy: 0.6530\n",
      "Epoch 6/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9870 - accuracy: 0.6506 - val_loss: 0.9611 - val_accuracy: 0.6633\n",
      "Epoch 7/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9377 - accuracy: 0.6691 - val_loss: 0.9439 - val_accuracy: 0.6635\n",
      "Epoch 8/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9048 - accuracy: 0.6786 - val_loss: 0.9275 - val_accuracy: 0.6747\n",
      "Epoch 9/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8702 - accuracy: 0.6909 - val_loss: 0.9074 - val_accuracy: 0.6835\n",
      "Epoch 10/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8399 - accuracy: 0.6982 - val_loss: 0.9062 - val_accuracy: 0.6910\n",
      "Epoch 11/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8135 - accuracy: 0.7087 - val_loss: 0.9089 - val_accuracy: 0.6883\n",
      "Epoch 12/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7847 - accuracy: 0.7179 - val_loss: 0.8926 - val_accuracy: 0.6892\n",
      "Epoch 13/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7615 - accuracy: 0.7239 - val_loss: 0.9238 - val_accuracy: 0.6892\n",
      "Epoch 14/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7385 - accuracy: 0.7339 - val_loss: 0.9155 - val_accuracy: 0.6881\n",
      "Epoch 15/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7171 - accuracy: 0.7402 - val_loss: 0.9076 - val_accuracy: 0.6934\n",
      "Epoch 16/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6941 - accuracy: 0.7467 - val_loss: 0.9052 - val_accuracy: 0.6932\n",
      "Epoch 17/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6760 - accuracy: 0.7529 - val_loss: 0.9224 - val_accuracy: 0.6889\n",
      "Epoch 18/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6530 - accuracy: 0.7623 - val_loss: 0.9348 - val_accuracy: 0.6863\n",
      "Epoch 19/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6353 - accuracy: 0.7674 - val_loss: 0.9428 - val_accuracy: 0.6921\n",
      "Epoch 20/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6218 - accuracy: 0.7728 - val_loss: 0.9501 - val_accuracy: 0.6950\n",
      "Epoch 21/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6069 - accuracy: 0.7775 - val_loss: 0.9592 - val_accuracy: 0.6897\n",
      "Epoch 22/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5928 - accuracy: 0.7809 - val_loss: 0.9675 - val_accuracy: 0.6910\n",
      "Epoch 23/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5700 - accuracy: 0.7906 - val_loss: 0.9920 - val_accuracy: 0.6911\n",
      "Epoch 24/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5586 - accuracy: 0.7926 - val_loss: 0.9995 - val_accuracy: 0.6919\n",
      "Epoch 25/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5465 - accuracy: 0.7984 - val_loss: 0.9963 - val_accuracy: 0.6930\n",
      "Epoch 26/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5293 - accuracy: 0.8033 - val_loss: 1.0498 - val_accuracy: 0.6816\n",
      "Epoch 27/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5185 - accuracy: 0.8072 - val_loss: 1.0507 - val_accuracy: 0.6913\n",
      "Epoch 28/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5021 - accuracy: 0.8141 - val_loss: 1.0604 - val_accuracy: 0.6910\n",
      "Epoch 29/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5004 - accuracy: 0.8132 - val_loss: 1.0558 - val_accuracy: 0.6904\n",
      "Epoch 30/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4894 - accuracy: 0.8165 - val_loss: 1.0498 - val_accuracy: 0.6953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f30342e8f98>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the student to achieve base accuracy\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "student.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "# 0.6932, 0.7047, 0.7026, 0.6953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-5_zIQhHEYW"
   },
   "outputs": [],
   "source": [
    "# Delete the model so that it can be retrained from scratch\n",
    "# Again, to achieve base accuracy.\n",
    "student.reset_states()\n",
    "student = None\n",
    "del student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Er5tVmYe-CIJ"
   },
   "source": [
    "### Hint and guided layer code\n",
    "\n",
    "In the code below the adaptation layer is created, and Wr is created as described in the paper. It is checked if the output of both the hint layer and the guided layer + adaptation layer is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "1jwpYYZDqhrp",
    "outputId": "7cd60662-9435-4635-96ec-d9901d8e0976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d_6/Identity:0\", shape=(None, 8, 8, 256), dtype=float32)\n",
      "Tensor(\"conv2d_13/Identity:0\", shape=(None, 8, 8, 256), dtype=float32)\n",
      "Tensor(\"activation/Identity:0\", shape=(None, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# conv_regressor is the adapation layer.\n",
    "conv_regressor = Conv2D(256, kernel_size=(1, 1),\n",
    "                 input_shape = (8,8,32),\n",
    "                 kernel_initializer='glorot_normal')(w_guided.output)\n",
    "\n",
    "w_r = Model(inputs=inputs, outputs=conv_regressor, name = 'w_r')\n",
    "print(w_hint.output)\n",
    "print(w_r.output)\n",
    "print(teacher.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUwC42o2-OMO"
   },
   "source": [
    "Creating the funciton below took me very long to figure out. In essence it takes the l2 distance between the guided layer and hint layer output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzzvis2NqlKi"
   },
   "outputs": [],
   "source": [
    "def fitnet_loss(target_feat, source_feat):\n",
    "    return tf.reduce_mean(tf.square(target_feat-source_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r6SV4l_f_u3Q",
    "outputId": "a8b0b821-3fae-472a-fa16-81ea02494040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 8, 8, 256)\n"
     ]
    }
   ],
   "source": [
    "w_r.compile(\n",
    "    optimizer='adam',\n",
    "    loss= (lambda y_hint, y_guided: fitnet_loss(y_hint, y_guided)))\n",
    "w_hint_outputs = w_hint.predict(X_train)\n",
    "print(w_hint_outputs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PwTF6i_0-Zzv"
   },
   "source": [
    "The hint outputs are then created, and the w_r trains the layers until the guided layer from the student model by taking the l2 distance between the hint output and the guided + adaptation layer output. Minimum loss is achieved within 9 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "noC8m4wSz6Me",
    "outputId": "4e17a48a-a4a3-49b6-e646-e5693703f54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0226\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0218\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0216\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0215\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0215\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0214\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0214\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0213\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0212\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf6f5e6a58>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_r.fit(X_train, w_hint_outputs,\n",
    "          batch_size=128,\n",
    "          epochs=10)#,\n",
    "          #verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2joFl43A-pZD"
   },
   "source": [
    "### Knowledge distillation code\n",
    "\n",
    "In the code below a manual softmax function is defined, and a new teacher model without its softmax output is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_Z9MA1GAkKL"
   },
   "outputs": [],
   "source": [
    "# Define a manual softmax function\n",
    "def softmax(x):\n",
    "    return np.exp(x)/(np.exp(x).sum())\n",
    "\n",
    "teacher_WO_Softmax = Model(teacher.input, teacher.get_layer('wo_softmax_teach').output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rE7vZ9Az--iL"
   },
   "outputs": [],
   "source": [
    "teacher_train_logits = teacher_WO_Softmax.predict(X_train)\n",
    "teacher_test_logits = teacher_WO_Softmax.predict(X_test) \n",
    "# This model directly gives the soft logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0RLLsNA_A0b"
   },
   "source": [
    "This output is then saved because otherwise I would have to retrain the teacher model each time I would want to train the student model with knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNwu8w3Jpu7u"
   },
   "outputs": [],
   "source": [
    "teacher_train_logits = np.load(\"teacher_train_logits.npy\")\n",
    "teacher_test_logits = np.load(\"teacher_test_logits.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16oA5qQu_3uk"
   },
   "source": [
    "The code below is rerun for all temperature values (2, 4, 6, 8, 10, 12 and 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVOqTX4oXN85"
   },
   "outputs": [],
   "source": [
    "# Set a tempature value\n",
    "temp = 12\n",
    "\n",
    "\n",
    "# Perform a manual softmax at raised temperature\n",
    "train_logits_T = teacher_train_logits / temp\n",
    "test_logits_T = teacher_test_logits / temp \n",
    "\n",
    "Y_train_soft = softmax(train_logits_T)\n",
    "Y_test_soft = softmax(test_logits_T)\n",
    "\n",
    "# Concatenate so that this becomes a 10 + 10 dimensional vector\n",
    "Y_train_new = np.concatenate([Y_train, Y_train_soft], axis=1)\n",
    "Y_test_new =  np.concatenate([Y_test, Y_test_soft], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "DhivsbAtxZPp",
    "outputId": "f4698f98-dd74-409b-916b-1c2741105527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 20)\n",
      "(10000, 20)\n",
      "(50000, 32, 32, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_new.shape)\n",
    "print(Y_test_new.shape)\n",
    "print(X_train.shape)\n",
    "Y_train_new[1]\n",
    "print(type(teacher_train_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzLr_PSV_q3I"
   },
   "source": [
    "Below the student model with a concatenation of two outputs is created, as described in the paper and Hinton's original knowledge distillation paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "V6JtC1fBxdhb",
    "outputId": "d9cd5146-c632-4c99-e837-2e7c603c9175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 16)   448         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 16)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 32)     0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          262272      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "wo_softmax_stud (Dense)         (None, 10)           1290        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10)           0           wo_softmax_stud[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 10)           0           wo_softmax_stud[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 10)           0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 20)           0           activation_3[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 268,650\n",
      "Trainable params: 268,650\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now collect the logits from the last layer\n",
    "logits = student.layers[-2].output # This is going to be a tensor. And hence it needs to pass through a Activation layer\n",
    "probs = Activation('softmax')(logits)\n",
    "\n",
    "# softened probabilities at raised temperature\n",
    "logits_T = tf.keras.layers.Lambda(lambda x: x / temp)(logits)\n",
    "probs_T = Activation('softmax')(logits_T)\n",
    "\n",
    "output = tf.keras.layers.concatenate([probs, probs_T])\n",
    "\n",
    "# This is the new student model\n",
    "student_kd = Model(student.input, output)\n",
    "\n",
    "student_kd.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGmEAE6GAGrO"
   },
   "source": [
    "Below the loss functions for knowledge distillation are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UeIixSPxh44"
   },
   "outputs": [],
   "source": [
    "# Declare knowledge distillation loss\n",
    "def knowledge_distillation_loss(y_true, y_pred, alpha):\n",
    "\n",
    "    # Extract the one-hot encoded values and the softs separately so that we can create two objective functions\n",
    "    y_true, y_true_softs = y_true[: , :nb_classes], y_true[: , nb_classes:]\n",
    "    \n",
    "    y_pred, y_pred_softs = y_pred[: , :nb_classes], y_pred[: , nb_classes:]\n",
    "    \n",
    "    loss = (1-alpha)*logloss(y_true,y_pred) + alpha*logloss(y_true_softs, y_pred_softs)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# For testing use regular output probabilities - without temperature\n",
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return tf.keras.metrics.categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bBVzr70ALy7"
   },
   "source": [
    "The codeblock below is run for all different values of alpha (0.3, 0.5, 0.7 and 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfSU6g8pD_5G"
   },
   "outputs": [],
   "source": [
    "student_kd.compile(\n",
    "    optimizer='adam',\n",
    "    loss=(lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, 0.5)),\n",
    "    metrics=[acc] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UlKcPI0Kxjbq",
    "outputId": "c2b07a9b-b05b-4573-c84b-40ec960b400d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7187 - acc: 0.4791 - val_loss: 0.6095 - val_acc: 0.5684\n",
      "Epoch 2/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5841 - acc: 0.5827 - val_loss: 0.5585 - val_acc: 0.6002\n",
      "Epoch 3/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5289 - acc: 0.6247 - val_loss: 0.5065 - val_acc: 0.6437\n",
      "Epoch 4/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4953 - acc: 0.6517 - val_loss: 0.4784 - val_acc: 0.6647\n",
      "Epoch 5/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4699 - acc: 0.6675 - val_loss: 0.4753 - val_acc: 0.6678\n",
      "Epoch 6/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4498 - acc: 0.6813 - val_loss: 0.4723 - val_acc: 0.6701\n",
      "Epoch 7/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4307 - acc: 0.6961 - val_loss: 0.4572 - val_acc: 0.6794\n",
      "Epoch 8/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4125 - acc: 0.7078 - val_loss: 0.4581 - val_acc: 0.6764\n",
      "Epoch 9/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3956 - acc: 0.7175 - val_loss: 0.4615 - val_acc: 0.6824\n",
      "Epoch 10/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3809 - acc: 0.7265 - val_loss: 0.4456 - val_acc: 0.6874\n",
      "Epoch 11/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3668 - acc: 0.7409 - val_loss: 0.4482 - val_acc: 0.6884\n",
      "Epoch 12/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3565 - acc: 0.7449 - val_loss: 0.4493 - val_acc: 0.6958\n",
      "Epoch 13/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3447 - acc: 0.7528 - val_loss: 0.4563 - val_acc: 0.6905\n",
      "Epoch 14/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3298 - acc: 0.7642 - val_loss: 0.4488 - val_acc: 0.6943\n",
      "Epoch 15/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3234 - acc: 0.7671 - val_loss: 0.4565 - val_acc: 0.6941\n",
      "Epoch 16/30\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.3144 - acc: 0.7737 - val_loss: 0.4594 - val_acc: 0.6933\n",
      "Epoch 17/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3035 - acc: 0.7804 - val_loss: 0.4683 - val_acc: 0.6930\n",
      "Epoch 18/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2934 - acc: 0.7887 - val_loss: 0.4784 - val_acc: 0.6894\n",
      "Epoch 19/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2867 - acc: 0.7923 - val_loss: 0.4734 - val_acc: 0.6939\n",
      "Epoch 20/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2797 - acc: 0.7979 - val_loss: 0.4799 - val_acc: 0.6911\n",
      "Epoch 21/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2763 - acc: 0.7983 - val_loss: 0.5068 - val_acc: 0.6828\n",
      "Epoch 22/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2632 - acc: 0.8084 - val_loss: 0.4878 - val_acc: 0.6900\n",
      "Epoch 23/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2602 - acc: 0.8120 - val_loss: 0.4931 - val_acc: 0.6907\n",
      "Epoch 24/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2504 - acc: 0.8178 - val_loss: 0.5014 - val_acc: 0.6934\n",
      "Epoch 25/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2462 - acc: 0.8217 - val_loss: 0.5035 - val_acc: 0.6930\n",
      "Epoch 26/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2399 - acc: 0.8246 - val_loss: 0.5164 - val_acc: 0.6900\n",
      "Epoch 27/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2335 - acc: 0.8285 - val_loss: 0.5124 - val_acc: 0.6987\n",
      "Epoch 28/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2265 - acc: 0.8342 - val_loss: 0.5414 - val_acc: 0.6900\n",
      "Epoch 29/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2242 - acc: 0.8358 - val_loss: 0.5343 - val_acc: 0.6953\n",
      "Epoch 30/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2218 - acc: 0.8370 - val_loss: 0.5422 - val_acc: 0.6892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdeb840a278>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_kd.fit(X_train, Y_train_new,\n",
    "          batch_size=64,\n",
    "          epochs=30,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYSeD27gAovO"
   },
   "source": [
    "The models had to be trained tens of times, thus the code blocks below were used to reset the weights. Both had to be run in order to recreate an initialised student_kd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBrRs_Ezq8F4"
   },
   "outputs": [],
   "source": [
    "student_kd.reset_states()\n",
    "student_kd = None\n",
    "del student_kd\n",
    "tf.keras.backend.clear_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcmQnTJ-JSqK"
   },
   "outputs": [],
   "source": [
    "student.reset_states()\n",
    "student = None\n",
    "del student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUEfe24pmx0x"
   },
   "source": [
    "### Teacher-assistant\n",
    "The code block below defines the student model for the multi-step knowledge distillation. The student here is called the pupil. As the results were unsatisfactory, I removed the hint layer code. The process of creating this code is equivalent precisely to what is done in the code above.\n",
    "\n",
    "### Dense layers\n",
    "I also removed the dense layer code, as these also led to unsatisfactory results. As described in the paper, the final test accuracy would always oscillate between two different values, whether they were trained on soft outputs or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "_HUcZzQftcG8",
    "outputId": "3221148c-c028-4635-92dd-95efa1b99137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pupil\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 32, 32, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                131104    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "wo_softmax_pupil (Dense)     (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 131,882\n",
      "Trainable params: 131,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "\n",
    "p_conv1 = Conv2D(16, kernel_size=(3, 3),\n",
    "                 activation='relu', \n",
    "                 padding = 'same', \n",
    "                 kernel_initializer='he_normal')(inputs)\n",
    "p_maxpool1 = MaxPooling2D(pool_size=(2, 2))(p_conv1)\n",
    "\n",
    "p_flat1 = Flatten()(p_maxpool1)\n",
    "p_dense1 = Dense(32, activation='relu')(p_flat1)\n",
    "p_do1 = Dropout(0.2)(p_dense1)\n",
    "p_dense_final = Dense(nb_classes, name = 'wo_softmax_pupil')(p_do1)\n",
    "p_softmax = Activation('softmax')(p_dense_final)\n",
    "\n",
    "pupil = Model(inputs=inputs, outputs=p_softmax, name = 'pupil')\n",
    "\n",
    "pupil.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "pupil.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdrkDQd5fLT3"
   },
   "outputs": [],
   "source": [
    "pupil.fit(X_train, Y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BdeyoKLOeuNc"
   },
   "outputs": [],
   "source": [
    "student_WO_Softmax = Model(student.input, student.get_layer('wo_softmax_').output)\n",
    "\n",
    "student_train_logits = student_WO_Softmax.predict(X_train)\n",
    "student_test_logits = student_WO_Softmax.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "fUxV0yDt5FdD",
    "outputId": "99ee2b5a-f885-403c-f1dd-0d5225968b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0592 - acc: 0.2165 - val_loss: 0.9253 - val_acc: 0.3236\n",
      "Epoch 2/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9504 - acc: 0.2835 - val_loss: 0.8609 - val_acc: 0.3848\n",
      "Epoch 3/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9176 - acc: 0.3042 - val_loss: 0.8358 - val_acc: 0.3714\n",
      "Epoch 4/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9004 - acc: 0.3145 - val_loss: 0.8244 - val_acc: 0.3899\n",
      "Epoch 5/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8889 - acc: 0.3200 - val_loss: 0.8219 - val_acc: 0.3787\n",
      "Epoch 6/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8813 - acc: 0.3209 - val_loss: 0.7919 - val_acc: 0.4005\n",
      "Epoch 7/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8734 - acc: 0.3238 - val_loss: 0.7907 - val_acc: 0.3961\n",
      "Epoch 8/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8684 - acc: 0.3263 - val_loss: 0.7961 - val_acc: 0.3990\n",
      "Epoch 9/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8627 - acc: 0.3259 - val_loss: 0.7824 - val_acc: 0.3966\n",
      "Epoch 10/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8606 - acc: 0.3276 - val_loss: 0.7960 - val_acc: 0.3840\n",
      "Epoch 11/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8575 - acc: 0.3324 - val_loss: 0.7886 - val_acc: 0.3871\n",
      "Epoch 12/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8559 - acc: 0.3327 - val_loss: 0.7882 - val_acc: 0.3938\n",
      "Epoch 13/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8524 - acc: 0.3339 - val_loss: 0.7670 - val_acc: 0.4077\n",
      "Epoch 14/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8505 - acc: 0.3336 - val_loss: 0.7776 - val_acc: 0.3946\n",
      "Epoch 15/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8483 - acc: 0.3363 - val_loss: 0.7620 - val_acc: 0.4073\n",
      "Epoch 16/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8456 - acc: 0.3350 - val_loss: 0.7675 - val_acc: 0.4054\n",
      "Epoch 17/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8425 - acc: 0.3386 - val_loss: 0.7638 - val_acc: 0.4066\n",
      "Epoch 18/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8395 - acc: 0.3393 - val_loss: 0.7704 - val_acc: 0.4007\n",
      "Epoch 19/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8378 - acc: 0.3401 - val_loss: 0.7638 - val_acc: 0.4045\n",
      "Epoch 20/20\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8367 - acc: 0.3391 - val_loss: 0.7659 - val_acc: 0.4021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f65ae4a4588>"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a tempature value\n",
    "temp = 8\n",
    "\n",
    "\n",
    "# Perform a manual softmax at raised temperature\n",
    "student_train_logits_T = student_train_logits / temp\n",
    "student_test_logits_T = student_test_logits / temp \n",
    "\n",
    "Y_train_soft_student = softmax(student_train_logits_T)\n",
    "Y_test_soft_student = softmax(student_test_logits_T)\n",
    "\n",
    "# Concatenate so that this becomes a 10 + 10 dimensional vector\n",
    "Y_train_new_student = np.concatenate([Y_train, Y_train_soft_student], axis=1)\n",
    "Y_test_new_student =  np.concatenate([Y_test, Y_test_soft_student], axis =1)\n",
    "\n",
    "# Now collect the logits from the last layer\n",
    "logits_pupil = pupil.layers[-2].output # This is going to be a tensor. And hence it needs to pass through a Activation layer\n",
    "probs_pupil = Activation('softmax')(logits_pupil)\n",
    "\n",
    "# softened probabilities at raised temperature\n",
    "logits_T_pupil = tf.keras.layers.Lambda(lambda x: x / temp)(logits_pupil)\n",
    "probs_T_pupil = Activation('softmax')(logits_T_pupil)\n",
    "\n",
    "output_pupil = tf.keras.layers.concatenate([probs_pupil, probs_T_pupil])\n",
    "\n",
    "# This is our new student model\n",
    "pupil_kd = Model(pupil.input, output_pupil)\n",
    "\n",
    "\n",
    "p_optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01, momentum=0.9, nesterov=True, name='SGD'\n",
    ")\n",
    "\n",
    "\n",
    "pupil_kd.compile(\n",
    "    #optimizer=tf.keras.optimizers.SGD(lr=1e-1, momentum=0.9, nesterov=True),\n",
    "    optimizer='adam',\n",
    "    loss=(lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, 0.5)),\n",
    "    #loss='categorical_crossentropy',\n",
    "    metrics=[acc] )\n",
    "\n",
    "\n",
    "pupil_kd.fit(X_train, Y_train_new_student,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test_new_student))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VTXF80K4hqM"
   },
   "outputs": [],
   "source": [
    "pupil.reset_states()\n",
    "pupil = None\n",
    "del pupil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hoLUaxxh47xl"
   },
   "outputs": [],
   "source": [
    "pupil_kd.reset_states()\n",
    "pupil_kd = None\n",
    "del pupil_kd"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "KD10cifar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
